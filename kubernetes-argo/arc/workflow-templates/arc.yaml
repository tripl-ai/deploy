apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: arc
spec:
  templates:
  - name: arc
    inputs:
      # override defaults here
      parameters:
        - name: jobId
        - name: configUri
        - name: serviceAccountName
        - name: environment
        - name: image
          value: "ghcr.io/tripl-ai/arc:arc_3.8.2_spark_3.0.2_scala_2.12_hadoop_3.2.0_1.0.0"
        - name: executorCores
          value: "1"
        # as integer gigabytes
        - name: executorMemory
          value: "1"
        - name: sparkConf
          value: ""
        - name: tags
          value: ""
        - name: parameters
          value: ""
        - name: sparkSqlShufflePartitions
          value: "32"
        - name: retryStrategy
          value: "2"
        - name: pullPolicy
          value: "Always"
    metadata:
      labels:
        workflowId: "{{workflow.uid}}"
    podSpecPatch: |
      serviceAccountName: "{{inputs.parameters.serviceAccountName}}"
      containers:
        - name: main
          resources:
            requests:
              cpu: "{{inputs.parameters.executorCores}}"
              memory: "{{inputs.parameters.executorMemory}}Gi"
    retryStrategy:
      limit: "{{inputs.parameters.retryStrategy}}"
      retryPolicy: "Always"
    script:
      image: "{{inputs.parameters.image}}"
      command: ["/bin/sh"]
      source: |
        # verbose logging
        set -ex

        # print current hostname and ip
        hostname
        hostname -I

        # submit job
        # driver memory is set at 90% of executorMemory
        bin/spark-submit \
        --master local[{{inputs.parameters.executorCores}}] \
        --driver-memory $(({{inputs.parameters.executorMemory}} * 1024 * 90/100))m \
        --driver-java-options "-XX:+UseG1GC" \
        --class ai.tripl.arc.ARC \
        --name arc \
        --conf spark.driver.host=$(hostname -I)  \
        --conf spark.driver.pod.name=$(hostname) \
        --conf spark.io.encryption.enabled=true \
        --conf spark.network.crypto.enabled=true \
        --conf spark.ui.enabled=false \
        --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=CORRECTED \
        --conf spark.sql.shuffle.partitions={{inputs.parameters.sparkSqlShufflePartitions}} \
        {{inputs.parameters.sparkConf}} \
        local:///opt/spark/jars/arc.jar \
        --etl.config.uri={{inputs.parameters.configUri}} \
        --etl.config.job.id={{inputs.parameters.jobId}} \
        --etl.config.environment={{inputs.parameters.environment}} \
        --etl.config.tags="service=arc workflowId={{workflow.uid}} pod={{pod.name}} serviceAccount={{inputs.parameters.serviceAccountName}} namespace={{workflow.namespace}} retries={{retries}} {{inputs.parameters.tags}}" \
        --ETL_CONF_DATE=${ETL_CONF_DATE} --ETL_CONF_EPOCH=${ETL_CONF_EPOCH} --ETL_CONF_TIMESTAMP=${ETL_CONF_TIMESTAMP} \
        --ETL_CONF_ENVIRONMENT={{inputs.parameters.environment}} {{inputs.parameters.parameters}}

  - name: arcCluster
    inputs:
      # override defaults here
      parameters:
        - name: jobId
        - name: configUri
        - name: serviceAccountName
        - name: environment
        - name: image
          value: "ghcr.io/tripl-ai/arc:arc_3.8.2_spark_3.0.2_scala_2.12_hadoop_3.2.0_1.0.0"
        - name: executorCores
          value: "1"
        # as integer gigabytes
        - name: executorMemory
          value: "1"
        - name: sparkConf
          value: ""
        - name: tags
          value: ""
        - name: parameters
          value: ""
        - name: sparkSqlShufflePartitions
          value: "32"
        - name: retryStrategy
          value: "2"
        - name: pullPolicy
          value: "Always"
    metadata:
      labels:
        workflowId: "{{workflow.uid}}"
    retryStrategy:
      limit: "{{inputs.parameters.retryStrategy}}"
      retryPolicy: "Always"
    script:
      resources:
        requests:
          cpu: "1"
          memory: "1Gi"
      image: "{{inputs.parameters.image}}"
      command: ["/bin/sh"]
      source: |
        # verbose logging
        set -ex

        # print current hostname and ip
        hostname
        hostname -I

        # submit job
        bin/spark-submit \
        --master k8s://kubernetes.default.svc:443 \
        --deploy-mode client \
        --class ai.tripl.arc.ARC \
        --name arc \
        --conf spark.authenticate=true \
        --conf spark.driver.extraJavaOptions="-XX:+UseG1GC" \
        --conf spark.driver.host=$(hostname -I)  \
        --conf spark.driver.memory=921m \
        --conf spark.driver.pod.name=$(hostname) \
        --conf spark.executor.cores={{inputs.parameters.executorCores}} \
        --conf spark.executor.extraJavaOptions="-XX:+UseG1GC" \
        --conf spark.executor.instances={{inputs.parameters.executorInstances}} \
        --conf spark.executor.memory={{inputs.parameters.executorMemory}}G \
        --conf spark.io.encryption.enabled=true \
        --conf spark.kubernetes.authenticate.caCertFile=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName={{workflow.serviceAccountName}} \
        --conf spark.kubernetes.authenticate.oauthTokenFile=/var/run/secrets/kubernetes.io/serviceaccount/token \
        --conf spark.kubernetes.container.image.pullPolicy={{inputs.parameters.pullPolicy}} \
        --conf spark.kubernetes.container.image={{inputs.parameters.image}} \
        --conf spark.kubernetes.driver.limit.cores=1 \
        --conf spark.kubernetes.driver.pod.name=$(hostname) \
        --conf spark.kubernetes.executor.label.workflowId={{workflow.uid}} \
        --conf spark.kubernetes.executor.limit.cores={{inputs.parameters.executorCores}} \
        --conf spark.kubernetes.executor.podNamePrefix=$(hostname)-spark \
        --conf spark.kubernetes.executor.request.cores={{inputs.parameters.executorCores}} \
        --conf spark.kubernetes.local.dirs.tmpfs=false \
        --conf spark.kubernetes.namespace={{workflow.namespace}} \
        --conf spark.network.crypto.enabled=true \
        --conf spark.ui.enabled=true \
        {{inputs.parameters.sparkConf}} \
        local:///opt/spark/jars/arc.jar \
        --etl.config.uri={{inputs.parameters.configUri}} \
        --etl.config.job.id={{inputs.parameters.jobId}} \
        --etl.config.environment={{inputs.parameters.environment}} \
        --etl.config.tags="service=arc workflowId={{workflow.uid}} pod={{pod.name}} serviceAccount={{inputs.parameters.serviceAccountName}} namespace={{workflow.namespace}} retries={{retries}} {{inputs.parameters.tags}}" \
        --ETL_CONF_DATE=${ETL_CONF_DATE} --ETL_CONF_EPOCH=${ETL_CONF_EPOCH} --ETL_CONF_TIMESTAMP=${ETL_CONF_TIMESTAMP} \
        --ETL_CONF_ENVIRONMENT={{inputs.parameters.environment}} {{inputs.parameters.parameters}}